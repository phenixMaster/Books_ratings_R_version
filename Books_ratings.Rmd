---
title: #"Books_ratings"
author: "Christophe"
output:
  html_document: 
    toc: yes
    number_sections: yes
---
# Books ratings data analysis 
## Background / context 
Book rating is important or critical for books lovers, writers and readers. Very 
often for some of them,ratings provides the reader with some first insights about
books and related features like author, publisher, category and guide the reader 
to buy, be tempt to or subject to buy a book or not. In this application we have been 
instructed to predict any given new book ratings  via a machine learning application mean. 
.    

## summary 
This application is a supervised machine learning problem. Its could be tackled either as a regression or a classification problem. we would follow the following flow :  
1 . data acquisition   
2. data inspection  
3. Data pre-processing or processing  
4. EDA : visualization  
5. features engineering  
6 Modelling and machine learning  
7. model testing and selection  
8. model performance  
9. prediction 
10. recommendation  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```


```{r clear_workspace,eval=FALSE}
rm(list=ls())
```

libraries 
```{r load_libraries,message=FALSE}
library(tidyverse)
library(skimr)
library(lubridate)
library(caret)
library(caretEnsemble)
```

## data acquisition
load data 
```{r load_data, message=FALSE, warning=FALSE}
data <- read_csv("books.csv")
```

## data inspection
```{r skim_data, paged.print=FALSE}
summary(skim(data) )
```

```{r attach_data, message=FALSE, warning=FALSE}
attach(data)
```
## Data preprocessing or processing  

## EDA 
Hist average rating  
```{r hist_average_ratings}
hist(average_rating,breaks = 10, probability  = T)
```
Density average rating  
```{r}
dens_AR <- density(data$average_rating)
plot(dens_AR,frame = F,main ="density av raing",xlab ="average rating")
```

cdf av rating   
```{r ecdf_quant_average_ratings}
Fn_AR <- ecdf(average_rating)
plot(Fn_AR, main="cdf average rating")
```
  
On average 60% of books have been rated less than  4.  
average rating is distributed around 4.  
summary av rating  
```{r summary_av_ratings}
summary(average_rating)
```

num pages   
```{r hist_num_pages}
hist(num_pages,breaks = 100)

```

summary num pages  
```{r summary_num_pages}
summary(num_pages)
```

hist text reviews  
```{r hist_text_reviews_pages}
hist(text_reviews_count, breaks = 50,probability = TRUE)

```

summary text reviews  
```{r summary_text_reviews_count}
summary(text_reviews_count)
```

hist ratings counts   
```{r hist_ratings_count}
hist(ratings_count, breaks = 50, prob = T)
```

summary ratings count  
```{r}
summary(data$ratings_count)
```

Let's discretize  average rating  
```{r categorizing_average_ratings}
nbClasses = 5
data$av_cat <- cut(average_rating,breaks = seq(0,5,by = 5/nbClasses),closed = "right", include.lowest = T
                         )
 barplot(table(data$av_cat),
         main = sprintf("categorized average ratings in %d classes",
                        nbClasses ))
```   

Discrete average rating ecdf  
```{r average rating ecdf}
Fn_AR <- ecdf(data$av_cat)
plot(Fn_AR, main = "cdf categorized average rating", frame = F)
```  
  
Discrete average rating frequencies table  
```{r summary_cat_av_ratings}
table(data$av_cat)
```

Discretizing average rating in ```r nbClasses``` classes  turn it heavily unbalanced.  

Density table Discrete Average rating   
```{r proportion_table_in_av_ratings}
round(prop.table(table(data$av_cat)),3)
```


There is  98% chance that a book  is rated between 3 and 5

```{r cum_density }
cumsum(round(prop.table(table(data$av_cat)),1))
```

There is 60% chance that average rating is less than or equal to 4. 

analysis of categorical variable

```{r select_cat_var, message=FALSE}
qualitative_data <- data %>% 
    discard(is.numeric) 
head(qualitative_data)
```

qualitative data names.  
```{r}
names(qualitative_data)
```

number of unique title  
```{r}
length(unique(title ))
```
  
what are the 5 most rated  titles?  
```{r title}
data %>% 
  select(title,average_rating) %>% 
  group_by(title) %>% 
  arrange(desc(average_rating)) %>% 
  head(5)
```

number of unique authors. 
```{r}
length(unique(authors))
```

What are the five authors with high rated books ?
```{r authors}
data %>% 
  select(authors,average_rating) %>% 
  group_by(authors) %>% 
  summarize( mean_avg_rating = mean(average_rating)) %>% 
  arrange ( desc(mean_avg_rating)) %>% 
  head(5)
```


What are the 5 languages that occur the most ?
```{r languages_code}
data %>% 
  select(language_code,average_rating) %>% 
  group_by(language_code) %>% 
  summarise(num_books_published = n()) %>% 
  arrange (desc(num_books_published)) %>% 
  head(5)
```
What are the 5 publishers with most high rated books ?
```{r publisher}
data %>% 
  select(publisher,average_rating) %>% 
  group_by(publisher) %>% 
  summarize( mean_avg_rating = mean(average_rating)) %>% 
  arrange (desc(mean_avg_rating)) %>% 
  head(5)
```
Academia press is the most productive publisher. 
 
Which year displays the highest number of book published ?
Let's check date format

cleaning publication date  first 
```{r, echo =F,results='hide'}
data$publication_date[c(8181 ,11099)]
```

clean publication date

```{r, echo = F,results='hide'}
cnt = 0
data$year <- vector(mode = "numeric",length(data$publication_date))
for (i in 1:length(data$publication_date)){
#i = 8181
  date0 <- data$publication_date[i]
  date0_vect <- str_c(unlist(str_split(date0,"/")))
    
  if (date0_vect[1]  %in% c("4","6","9","11") & as.numeric(date0_vect[2]) > 30) {
    date0 <- str_c(date0_vect[1],"30",date0_vect[3],sep ="-")
   # day(date0) <- 30
    
  }
  else{
    date0 <- str_c(date0_vect[1],date0_vect[2],date0_vect[3],sep ="-")  
  }
  data$year[i] <- as.numeric(date0_vect[3])
  data$publication_date[i] <- date0
cnt = cnt + 1
}

# check for bad formats records 
data$publication_date[c(8181 ,11099)]
```
```{r, echo = F,results='hide'}
head(data$publication_date)
tail(data$publication_date)
```

```{r hist year1}
hist(data$year,
     breaks = 100,
     xlab = "year",
     main ="publication date year is left skewed")
```

```{r hist year 2}
hist_year <-hist(data$year,
     breaks = 100,
     xlim = c(1980,2010),
     main = "histogram of publication year from 1980",
     xlab = "year")


```


```{r year }
data %>% 
  group_by(year) %>% 
  summarise(counts = n()) %>% 
  arrange(desc(counts)) %>% 
  head(1)

summary(data$year)
```


Year 2006 yields the  highest number of publications i.e. 1700. However, in this record, 75% of books have been published by 2005.   


2D EDA 

What is the number of book published by year and their associated average rating  
```{r}
data %>% 
  group_by(year) %>% 
  summarize(num_books_published = n(),avg_rating = mean(average_rating)) %>% 
  arrange(desc(num_books_published))
```

how is average rating distributed across the years?  
```{r year vs av rating}
data %>% 
  ggplot(aes(year,average_rating, 
             color = between(average_rating ,3,4.7) & between(year,1975,2010) )) + 
  geom_point() +
  labs(title = "average rating between 3 and 4.7 for years range 1900 to 2020",
       color = "average rating between 3 and 4.7 and
       year between 1975 and 2005")
```

In which years do average rating fall in the range of 2.7 and 4.5?    
```{r year vs av ratings}
# categorizing year
data$year_cat <- cut_width(data$year,width = 9)
data %>% 
  ggplot(aes(year_cat, average_rating, 
             fill = between(average_rating,2.7,4.5))) + geom_boxplot() +
  labs(y = "average rating",fill = "average rating range") +
  theme(axis.text.x  = element_text (hjust = -.1, angle = -45))
```

In which range did average rating fall between 1990 and 2020?  
```{r filter year after 1990}
data %>% 
  filter(between(year,1990,2020)) %>% 
  ggplot(aes(year_cat,average_rating)) + geom_boxplot() +
  labs(x = "year of publication 1990")
```

Most books are rated between 2.75 and 4.7.   

```{r}
data %>% 
  filter(year >= 1976) %>% 
  ggplot(aes(year_cat,average_rating)) + geom_boxplot() + 
  labs(x = "year of publication above 1976")
```

Across years , most books are rated between 2.7 and 4.5. 


```{r}
data %>% group_by(year) %>% 
  summarize(across(.cols = c(average_rating,ratings_count),
                                 funs = mean))
```


select data 
```{r}
data_num0<- data %>% 
  keep(is.numeric) %>% 
  select(-c(bookID,average_rating,year)) %>% 
  apply(FUN = function(x) x/max(x),MARGIN = 2) %>% 
  as_tibble
data_num <- cbind(data_num0,
                  average_rating = data$average_rating,
                  year = data$year)  
```

```{r}
head(data_num)
```
```{r}
hist(data_num$ratings_count ,probability  = T)
```

```{r paged.print=TRUE}
#skim(data_num)
dim(data)
dim(data_num)
```


IMPORTATNT : We could normalize data by their maximum to diminish the effect of
magnitude in some numeric variables. 
normalize numeric data 
```{r,eval=T}
data_num <- data_num %>% 
  select(c(num_pages,ratings_count,text_reviews_count)) %>% 
  apply(.,FUN = scale,MARGIN = 2) %>% 
  cbind(year = data_num$year,average_rating = data_num$average_rating) %>% 
  as.data.frame()
head(data_num)
```

## features engineering 
- Removes some features without variation or. very low variation or without any relation. with average ratings.   
hypothesis testing : here we test the relationship between average rating and num variables   
H0 : there is no relationship between average_rating and numerical variables   
H1 : there is a relationship between average_ratings and numerical variables. 

split data 
```{r}
set.seed(1243)
index_train0 <- sample(1:(dim(data_num)[1]),round(0.8*(dim(data_num)[1])), replace = F)
index_train1 <- sample(index_train0,round(0.8*length(index_train0)), replace = F)
index_val <- sample(index_train0,round(0.2*length(index_train0)), replace = F)
```


```{r}
train1 <- data_num[index_train1,]
valid <- data_num[index_val,]
test <- data_num[-index_train0,]
```

```{r}
broom::tidy(lm(average_rating~. , train1)) %>% 
  filter(p.value <= 0.05)
```
For ratings_count, text_reviews_count, year , the p-values are > 
0.05. We reject H1 for some variables as they have  no relationship with average_rating and keep num_pages.  

```{r,eval = F}
data_num <- data %>% 
  select(num_pages)
```

```{r data2}
data2 <- data_num %>% 
  #discard(is.numeric) %>% 
  #cbind(num_pages = data_num$num_pages,average_rating = data$average_rating) %>% 
  select(-c(contains(c("ID","title","isbn","pub","auth","year","cat"))))

head(data2)
```
refactor language code 
```{r}
data2$language_code<- ifelse(str_detect(toupper(data$language_code),toupper("eng")),1,0)
```


```{r}
table(data2$language_code)
```

```{r}
broom::tidy(summary(lm(average_rating~. , data2)) ) %>% 
  filter(p.value <= 0.05)
```
```{r}
data3 <- data %>% 
  #discard(is.numeric) %>% 
  #cbind(num_pages = data_num$num_pages,average_rating = data$average_rating) %>% 
  select(-c(contains(c("ID","title","isbn","pub","auth","year","average"))))
```

split data again

## modelling testing and selection   
- Test some models and make a choice to finalize our study. 
- test categorical average rating.  
```{r}

randomForest::randomForest (av_cat~., data3 )
```

## model performance    

## recommendation if necessary  

```{r}
# tree
# rf
```

