---
title: #"Books_ratings"
author: "Christophe"
output:
  html_document: 
    toc: true
    toc_float: true
    number_sections: yes
---
# Books ratings data analysis 
## Background / context 
Book rating is important or critical for books lovers, writers and readers. Very 
often for some of them,ratings provide the reader with some first insights about
books and related features like author, publisher, category and guide the reader 
to buy, be tempt to or subject to buy it or not. In this application we have been 
instructed to predict any given new book ratings  via a machine learning application mean. 
.    

## summary 
This application is a supervised machine learning problem. Its could be tackled either as a regression or a classification problem. we would follow the following ML flow :  
1. Data testing  
1.1. data acquisition   
1.2. data inspection  
1.3. Data pre-processing or processing   
1.4. EDA : visualization   
1.5. features engineering   
2. Model testing. 
2.1. Model testing  
2.2. model evaluation  
2.3. model validation   
3. Predictions   
4. recommendation   

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```


```{r clear_workspace,eval=FALSE}
# clear workspace
rm(list=ls())
```

libraries 
```{r load_libraries,message=FALSE}
library(tidyverse)
library(skimr)
library(lubridate)
library(caret)
library(caretEnsemble)
library(broom)
library(e1071)
library(stats)
library(xgboost)
library(ROSE)
```

## Data testing
### data acquisition
load data 
```{r load_data, message=FALSE, warning=FALSE}
data <- read_csv("books.csv")
```

### data inspection
data overview  
```{r skim_data, paged.print=FALSE}
# custom skim function for summary stats 
my_skim <- skim_with(numeric = sfl(median, 
                                   iqr = IQR, 
                                   p0= NULL,
                                   p25 = NULL,
                                   p50 = NULL,
                                   p75 = NULL,
                                   p100= NULL,
                                   hist = NULL),
                     character = sfl(length =length)
                     )

# skim data 
skimData <- my_skim(data)
summary(skimData )
```
deeper details 
```{r na check}
# missing data 
sum(skimData$n_missing)
```

There is no missing data in this dataset.  

```{r data partition summary stats table}
# partition data description 
data_Desc_Partition <- partition(skimData)
```

```{r character data summary stats}
data_Desc_Partition$character %>% 
  select(-n_missing)
```


 we can see that title has 10352 cases instead of 11127, which suggest that there are duplicates cases in title. 

This table yields, 6643 authors, for 11127 books,which represents almost 2 books written by each authors roughly. 

Publication_date has 3679 cases, this suggest that there are some point of time where a lot of book have been published.  


```{r sum stats numeric data }
data_Desc_Partition$numeric %>% 
  select(-n_missing)
```
Data magnitude are very high and different each other. 

Average_rating mean and median are close each to another other, this suggest that the variable distribution may be close to a normal one.  


```{r attach_data, message=FALSE, warning=FALSE}
attach(data)
```

### Data preprocessing or processing 

clean publication date

```{r, echo = F,results='hide'}
# inconsitent formated data
cat("bad format data:",
data$publication_date[c(8181 ,11099)]
)

cnt = 0
data$year <- vector(mode = "numeric",length(data$publication_date))
for (i in 1:length(data$publication_date)){
#i = 8181
  date0 <- data$publication_date[i]
  date0_vect <- str_c(unlist(str_split(date0,"/")))
    
  if (date0_vect[1]  %in% c("4","6","9","11") & as.numeric(date0_vect[2]) > 30) {
    date0 <- str_c(date0_vect[1],"30",date0_vect[3],sep ="-")
   # day(date0) <- 30
    
  }
  else{
    date0 <- str_c(date0_vect[1],date0_vect[2],date0_vect[3],sep ="-")  
  }
  data$year[i] <- as.numeric(date0_vect[3])
  data$publication_date[i] <- date0
cnt = cnt + 1
}

# check for bad formats records 
cat("check dates format consistency",
data$publication_date[c(8181 ,11099)]
)
```


```{r, echo = F,results='hide'}
head(data$publication_date)
tail(data$publication_date)
```

## EDA 
### 1D EDA : investigate variable distribution 

```{r plot hist}
adRefLines <- function(num_variable){
  if (is.numeric(num_variable)){
    #hist(num_variable,breaks,freq = F)
    abline(v = mean(num_variable),col = "red")
    abline(v = median(num_variable),col = "blue")
  }
  else stop()
}
```

```{r numerical summary average_ratings}
summary(average_rating)
```
Hist average rating  
```{r hist average_ratings}
#attach(data2)
hist(average_rating,breaks = 100, probability  = T)
adRefLines(average_rating)
```

Density plot of average rating  : interquartile range 
```{r Density average rating }
dens_AR <- density(average_rating)
plot(dens_AR,frame = FALSE,main ="density average rating",
     xlab ="average rating")

x = dens_AR$x
y = dens_AR$y
# define x as quartiles 1 and 3 
x_i = quantile(average_rating,.25)
x_f = quantile(average_rating,.75)
x_interval = x > x_i & x< x_f # boolean 
#y_i = 0
y_i = y[x_i]
y_f = y[x_f]
polygon( 
         x = c(x_i,x[x_interval],x_f),
         y =  c(y_i,y[x_interval],y_f),

         col = "lightblue")

```
 
 50 % of rating distribution is  between 3.78 and 4.17 (inter-quantile range).     
```{r ecdf_quant_average_ratings}
Fn_AR <- ecdf(average_rating)
plot(Fn_AR, main="cdf average rating", pch ='.', frame= FALSE)
abline(v = 4, col ="red")
abline(h = Fn_AR( 4), col ="blue")
```
  
The probability to have a book rating less or equal to  4 is almost 0.5.    

histogram of num pages   
```{r hist num_pages}
hist(num_pages,breaks = 100)
adRefLines(num_pages)

```
num_pages is right-skewed. Most number of pages are of high values or above the median value of num pages.  
summary num pages  
```{r numerical summary num_pages}
summary(num_pages)
```

histogram of text reviews count 
```{r hist text_reviews_count}
hist(text_reviews_count, breaks = 100,probability = TRUE)
adRefLines(text_reviews_count)
```
Text reviews count is right-skewed. There are many higher values in its distribution.  

numerical summary text_reviews_count
```{r summary_text_reviews_count}
summary(text_reviews_count)

cat("\n IQR text_reviews_count :",IQR(text_reviews_count))
```
On average, 25 up to 75% text reviews counts are between 9 to 237.5. 

Which book has text_reviews_count of 942665 ?
```{r}
title[which.max(text_reviews_count) ]
```

Twilight book1 is the book with the book with the highest text_reviews_count   

hist ratings counts   
```{r hist_ratings_count}
hist(ratings_count, breaks = 50, prob = T)
adRefLines(ratings_count)
```
The distribution is right skewed, many high values are present is ratings count distribution.  

summary ratings count  
```{r summary ratings counts}
summary(ratings_count)
```


Which book has ratings_count of max rating count : a suspect count ?
```{r mbook with max ratings_count }
title[which.max(ratings_count)]
```

Let's discretize  average rating  in five (5) classes   : bar plot 
```{r categorizing_average_ratings}
nbClasses = 5
data$av_cat <- cut(average_rating,breaks = seq(0,5,by = 5/nbClasses),
                   closed = "right", include.lowest = T
                         )
 barplot(table(data$av_cat),
         main = sprintf("categorized average ratings in %d classes",
                        nbClasses ))
```   
discrete average_rating yields also high frequencies  in (3-4] and (4-5] classes.  
Discrete average rating ecdf   
```{r average rating ecdf}
Fn_AR_cat <- ecdf(data$av_cat)
plot(Fn_AR_cat, main = "cdf categorized average rating", frame = FALSE)
abline(v = 4, col ="red")
abline(h = Fn_AR( 4), col ="blue")
```  

The probability to have an average rating less or equal to 4 is around or near to  equal to 0.5, once again.   
Most of the time average ratings are around  or above 4 in this distribution. 
 

Discrete average rating frequencies table  

Density table Discrete Average rating   
```{r proportion_table_in_av_ratings}
round(prop.table(table(data$av_cat)),3)
```

The proportion of books rated in class 3-4 is .56,  while it is .42 for 4-5 class.  
```{r proportion of av rating between class 3-5}
sum(round(prop.table(table(data$av_cat)),3)[4:5] ) %>% sum()
```

On average, 99% of books are  rated between 3 and 5

```{r cum_density }
F <- cumsum(round(prop.table(table(data$av_cat)),1))
F
cat("\n proportion of average rating greater or equal to  4\n")
as.numeric(F[5] - F[4])
```

Here, 60% of books received an average  rating less or equal to 4.  Discretizing average rating in ```r nbClasses``` classes  turn it heavily unbalanced.  

= analysis of categorical variable =  
```{r select_cat_var, message=FALSE}
qualitative_data <- data %>% 
    discard(is.numeric) 
head(qualitative_data)
```
qualitative data names.  

```{r qualitative data}
names(qualitative_data)
```

what are the 5 most high rated  titles on average?  
```{r title}
data %>% 
  select(title,average_rating) %>% 
  group_by(title) %>% 
  arrange(desc(average_rating)) %>% 
  head(5)
```

What are the five authors with high rated books ?
```{r authors}
data %>% 
  select(authors,average_rating) %>% 
  group_by(authors) %>% 
  summarize( mean_avg_rating = mean(average_rating)) %>% 
  arrange ( desc(mean_avg_rating)) %>% 
  head(5)
```
Aristophanes is the most prolific author.  

What are the 5 languages that occur the most ?
```{r languages_code}
data %>% 
  select(language_code,average_rating) %>% 
  group_by(language_code) %>% 
  summarise(num_books_published = n(), 
            prop = num_books_published/nrow(data)) %>% 
  arrange (desc(num_books_published)) %>% 
  head(5)
```

```{r numerical summary english}
str_detect(data$language_code,("^([Ee][Nn][g\\-])")) %>% mean %>%  round(2)
```

English is the language that occurs the most , with a proportion of 0.95, followed by spanish and french among the most 5 well-rated books.  

bin language code 
```{r refactor language_code}
# 1: book written in English 
# 0 : books not written in English

 data$language_code <- ifelse(str_detect(data$language_code,"^([Ee][Nn]*)"),1,0) %>% as.factor()

cat("proportion of binary language code :\n")
round(prop.table(table(data$language_code)),2)

```

What are the 5 publishers with most high rated books ?
```{r numerical summary of publisher}
data %>% 
  select(publisher,average_rating) %>% 
  group_by(publisher) %>% 
  summarize( mean_avg_rating = mean(average_rating)) %>% 
  arrange (desc(mean_avg_rating)) %>% 
  head(5)
```
Academia press is the most productive publisher, followed by Boosey & Hawkes Inc.  
 
Which year displays the highest number of book published ?
Let's check date format


which year display high rating frequencies ?  
```{r hist year1}
hist(data$year,
     breaks = 100,
     xlab = "year",
     main ="publication date year is left skewed")
adRefLines(data$year)
```

```{r hist year 2}
hist_year <-hist(data$year,
     breaks = 100,
     xlim = c(1980,2010),
     main = "histogram of publication year from 1980",
     xlab = "year")

adRefLines(data$year)
```

numerical summary of year : 
What is the year with the highest frequency of publication?  
```{r numerical summary year}
data %>% 
  group_by(year) %>% 
  summarise(counts = n()) %>% 
  arrange(desc(counts)) %>% 
  head(1)
```
### 1D EDA summary  
Books have been published from 1900 to 2020. 
Year 2006 yields the  highest frequency of published books i.e. 1700. 

1D section summary :  
We found that :
* Average rating and all numerical variables present outliers. 
* The proportion of book rated on average between 3-4 is up to .56. 
* The proportion of book rated on average between 3-5 is up to .99. 
* year 2006 yields a high frequency rating 
* English is the language which occurs the most, with a proportion of  0.96


###  2D EDA 
== Correlation ===
which numerical variables are correlated ?
```{r correlated data }
corr_data <- data %>% select(where(is.numeric)) %>% 
   cor %>% 
   as.data.frame() 
corr_data
(corr_data >=.15 & corr_data < 1)*1 
```
What is the number of book published by year and their associated average rating  
```{r AR vs number of book published}
data %>% 
  group_by(year) %>% 
  summarize(num_books_published = n(),avg_rating = mean(average_rating)) %>% 
  arrange(desc(num_books_published)) %>% 
  head(5)
```

how is average rating distributed across the years ?  
```{r year vs av rating}
data %>% 
  ggplot(aes(year,average_rating, 
             color = between(average_rating ,3,4.7) & between(year,1975,2010) )) + 
  geom_point(position = "jitter") 
  labs(title = "average rating between 3 and 4.7 for years range 1900 to 2020",
       color = "average rating between 3 and 4.7 and
       year between 1975 and 2006")
```
high rated books ,had been published between 1975 and 2006. 

In which years do average ratings in the range of 2.7 and 4.5 fall?    
```{r year vs av ratings, eval = F}
# categorizing year
data$year_cat <- cut_width(data$year,width = 9)
data %>% 
  ggplot(aes(year_cat, average_rating, 
             fill = between(average_rating,2.7,4.5))) + geom_boxplot() +
  labs(y = "average rating",fill = "average rating range") +
  theme(axis.text.x  = element_text (hjust = -.1, angle = -45))
```

In which range did average rating fall between 1990 and 2020?  

Most books are rated between 2.75 and 4.7.   

```{r year vs av_rating_nested,echo=FALSE,eval =FALSE }
data %>% 
  filter(year >= 1976) %>% 
  ggplot(aes(year_cat,average_rating)) + geom_boxplot() + 
  labs(x = "year of publication above 1976")
```

Across years , most books are rated between 2.7 and 4.5. The distribution of average ratings in any year range is almost similar. 

what are the top 5 ratings counts  with highest mean average rating across years ?  
```{r summarize av rating and rating counts}
data %>% group_by(year) %>% 
  summarize(across(.cols = c(average_rating,ratings_count),
                                 .fns = list(mean = mean),
                    .names = ("{.col}_{.fn}"))) %>% 
                    arrange(desc(average_rating_mean)) %>% 
                    head(5)
```

```{r}
# correlation coef 
cor(average_rating ,ratings_count) %>% round(2)
```
The correlation coefficient between average rating and ratings count is 0.03, which suggest a very weak to no existing linear relationship between average rating and rating counts. 

```{r AR vs ratings count}
ggplot(data,aes(ratings_count ,average_rating)) +
    geom_point(position = "jitter") +
    geom_smooth() #+
    #scale_x_log10()
```
There is not a clear linear pattern between rating counts and average rating. 

```{r AR vs num_pages}
data %>% 
    ggplot(aes(num_pages,average_rating)) +
    geom_point(position = "jitter") +
    geom_smooth()# +
    #scale_x_log10()
```

There is a weak linear pattern between average rating and num_pages and a slightly non-linear pattern.  
```{r cor AR num_pages}
cor(average_rating, num_pages) %>% round(2) 
```

The correlation coefficient between average rating and num_pages is 0.15. This suggest a very weak linear relationship between average rating and num_pages. 

```{r text_reviews_count vs average_rating}
ggplot(data, aes(text_reviews_count, average_rating)) +
    geom_point() +
    geom_smooth() #+ 
    #scale_x_log10()
```

```{r num summary average rating vs text reviews count}
cor(average_rating, text_reviews_count) %>%  round(2)
```

The correlation coefficient  between average rating and text_count_reviews is 0.03 suggesting a very weak linear relationship near to not existing between average rating and text reviews count. 

2D section summary :  
------------------  
No numerical variable has correlation coefficient  above 0.5 with average rating, though, considering a linear relationship, none of them has enough predictive power over average_ratings. 

We notice a high positive correlation (0.87), between rating_counts and text_reviews_count. This suggest that they are correlated hence introducing redundancy or convey the same information pattern. We would advise to remove one of it. 

Considering this results we suggest to investigate underlying non-linear relationships between average rating and remaining features.   

## features engineering 
We will be looking for variables having effective predictive power over average rating in a linear regression setting.  Then remove it in case of linear model setting in model testing.  
 

Let's split data and test default models.  
split data 
```{r split data 2}
# remove no variation data
data <- data %>% 
  #discard(is.numeric) %>% 
  #cbind(num_pages = data2$num_pages,average_rating = data2$average_rating) %>% 
  select(-c(contains(c("ID","title","isbn","pub","auth","year"))))
set.seed(1243)
index_train0 <- sample(1:(dim(data)[1]),round(0.8*(dim(data)[1])), replace = FALSE)
index_train1 <- sample(index_train0,round(0.8*length(index_train0)), replace = FALSE)
index_val <- sample(index_train0,round(0.2*length(index_train0)), replace = FALSE)
```


```{r split data v2}
dataPartitions <- createDataPartition(average_rating,
                                      times =1,
                                      p = 0.6,
                                      list = T
          )
```


```{r split data 3}
train1 <- data[index_train1,]
valid <- data[index_val,]
test <- data[-index_train0,]
```

= hypothesis testing for linear relation between target and numerical variable =     

we test the relationship between average rating and numerical variables.     
H0 : there is no relationship between average_rating and numerical variables   
H1 : there is a relationship between average_ratings and numerical variables. 

lm
```{r lm model,echo = TRUE }
# use list of methods
 lm <- train(average_rating~. - av_cat,
       method = "lm",
       data = train1
       )
```

linear model summary table
```{r}
lm <- lm(train1$average_rating~. -av_cat , train1) 
tidy(lm) %>% 
  filter(p.value <= 0.05)
```

remove text_reviews_count
```{r remove text_reviews_count}
data <- data %>% 
  select(-text_reviews_count)
```


SUMMARY:  
-For ratings_count, year , p-values < 0.05 for num_pages , and ratings_ count though we conclude that other variables has no effects/ no relationship or influence on average_rating in a linear multivariate regression application.  
0.05.Given that,  we would investigate a non linear relationship or model.    

-We will remove some categorical features without variation i.e. isbn, isbn13, bookID, authors, title or  that with very low variation or those without any relation with average ratings following the lm model. 
Also remove following qualitative variables,

Let's split data and test some non-linear models.  

## model testing 

- Test some models and make a choice to finalize our study on regression sub-section. 
- test categorical average rating against models and make a model choice if it is the case. 



== regression ==  
rf regression
lm
```{r lm,eval=FALSE}
lm <- train(average_rating~ num_pages + language_code, 
            method = "lm",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)
```


```{r rf model_regression, eval = F}

rf <- train(average_rating~. -av_cat, 
            method = "ranger",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)
```

lasso
```{r lasso,eval=FALSE}
lasso <- train(average_rating~. -av_cat, 
            method = "lasso",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)
```

ridge
```{r ridge,eval=FALSE}
ridge <- train(average_rating~. -av_cat, 
            method = "ridge",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)
```

gbm
```{r gbm}
set.seed(1234)
gbm <- train(average_rating~. -av_cat, 
            method = "gbm",
            trControl= trainControl(allowParallel = TRUE),
            verbose = F,
            data = train1)
```

```{r}
plot(gbm)
```

xgboost model 
```{r xgboost data split}
# create xgb DMatrix
createXgbMatrix <-function(df) {
   df <- df %>% 
        select_if(is.numeric)  %>% 
        #select(-c("av_cat","language_code")) %>% 
        as.matrix() %>% xgb.DMatrix(label = df$average_rating)

}

# xgb train DMatrix
xgb_train <- createXgbMatrix(train1)
 
# valid 
xgb_valid <- createXgbMatrix(valid)

# test
xgb_test <- createXgbMatrix(test)

```


```{r xgboost modelling }
# xgb.train
param = list()
watchlist = list(train = xgb_train,eval = xgb_valid)
xgb_reg_model <- xgb.train(#param,
                           data = xgb_train,
                            watchlist = watchlist,
                            nrounds = 1000,
                            verbose = 0
                   )
```


xgboost rmse model
```{r xgboost rmse}
# error 
xgb_res <- xgb_reg_model$evaluation_log %>% map(mean)

# model perf 
cat("xgboost model performance on valid set : \n")
xgb_res[2:3]
```
```{r}
plot(xgb_reg_model$evaluation_log$eval_rmse,pch = ".",
     col = "red",
     frame =FALSE,
     xlab = "num of iterations",
     ylab = "validation rmse")
lines(xgb_reg_model$evaluation_log$train_rmse, col = "blue")
```

This model tends to overfit. 
to be check...
```{r train control xgboost caret }
xgb_trcontrol = trainControl(seeds =set.seed(1234),
  #method = "cv",
 # number = 5,  
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)
```

```{r bstTree model caret}
# create train frame 
 X_train <- train1 %>% 
            select_if(is.numeric) %>% 
            select(-"average_rating") %>% 
            as.matrix() 
set.seed(1234) 
bstTree = train(X_train,
  train1$average_rating,  
  trControl = xgb_trcontrol,
  method = "bstTree"
)
```

```{r}
plot(bstTree)
```

```{r xgboost tree caret,eval = F}
# extrem gradient boosting tree
set.seed(0) 
xgbTree <- train(X_train,
  train1$average_rating,  
  trControl = xgb_trcontrol,
  method = "xgbTree"
)
```

svm
```{r svmLinear,eval=T}
svmLinear <- train(average_rating~. -av_cat, 
            method = "svmLinear",
            trControl= trainControl(allowParallel = TRUE),
            verbose = F,
            data = train1)
```

```{r,eval = FALSE}
plot(svmLinear)
```

```{r svmLinear2,eval = FALSE}
# svm2
svmLinear2 <- train(average_rating~. -av_cat, 
            method = "svmLinear2",
            trControl= trainControl(allowParallel = TRUE),
            verbose = F,
            svr_eps = .01,
            data = train1)
```

```{r,eval = FALSE}
plot(svmLinear2)
```

```{r svmLinear3,eval=FALSE,echo=FALSE}
# svm linear 3
svmLinear3 <- train(average_rating~. -av_cat, 
            method = "svmLinear3",
            trControl= trainControl(allowParallel = TRUE),
            verbose = F,
            svr_eps = .01,
            data = train1)
```



```{r resamples }
# compare method perfs
set.seed(1234)
res <- resamples(list(#"lm" = lm,
                      #"rf" = rf,
                      #"lasso" = lasso,
                      "gbm" = gbm,
                      #"ridge"= ridge,
                      "svmLinear" = svmLinear ,
                      #"svmLinear2" = svmLinear2 ,
                      "bstTree" = bstTree#,
                      #"xgbTree" = xgbTree
                      ),
                 metric = "RMSE",
                 decreasing = TRUE
                 )

# plot perf summary
ggplot(res) + labs(title = "models RMSE")
select(res$values, contains("Model")) %>% colMeans() 
```

```{r summarise methods perfs}
res_values <- data.frame(res$values) 

(res_MAE <- res_values %>% select(contains("MAE")) %>% 
    round(2) %>% 
    map(mean) %>% t() )
cat("\n")
(res_RMSE <- res_values %>% select(contains("RMSE"))%>%  
    round(2) %>% 
    map(mean) %>% t())
#(resVal <- cbind(res_MAE,res_RMSE) %>% map(mean) %>% as_tibble())
#head(resVal)
```
== Regression conclusion ===
Following this model testing, I would advise bstTree (boosting tree) or xgboost
(extreme gradient boosting machine ) models for a regression application for now.  

== classification ==  
considering average rating as categorical variable turn our application into a multi-class classification problem. We want to see if a classification application is a better choice for our application. 

I will  test algorithm and make a choice.  

```{r}
table (train1$av_cat)
```

== sampling data ==  

```{r, eval=FALSE, echo = FALSE}
classFolds <- groupKFold(group = data2$av_cat, k = length(unique(group)))
```

rf classification

```{r rf model, eval = FALSE, echo = TRUE}

rf_class <- train(av_cat~. -average_rating, 
            method = "ranger",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)
```


```{r, eval = FALSE}
#plot(rf_class)
```



```{r bagg, echo = FALSE, eval = FALSE}
# tree model 
bst_class <- train(av_cat~. -average_rating, 
            method = "bstTree",
            trControl= trainControl(allowParallel = TRUE),
            data = train1)

```

```{r, eval = FALSE}
plot(bst_class)
```


```{r boosting models, eval = FALSE,echo = FALSE}
set.seed(1234)
gbm_class <- train(av_cat~. -average_rating, 
            method = "gbm",
            trControl= trainControl(allowParallel = TRUE),
            verbose = FALSE,
            data = train1)
```

```{r, eval=FALSE}
plot(gbm_class)
```


```{r resamples_class,eval=FALSE}
set.seed(1234)
res <- resamples(list(#"lm" = lm,
                      #"rf_class" = rf_class,
                      #"lasso" = lasso,
                      "gbm_class" = gbm_class,
                      #"ridge"= ridge,
                      #"svmLinear" = svmLinear ,
                      #"svmLinear2" = svmLinear2 ,
                      "bstTree_class" = bst_class,
                      #"xgbTree" = xgbTree
                      ),
                 metric = "Accuracy",
                 decreasing = FALSE
                 )

# plot perf summary
ggplot(res) + labs(title = "models Accuracy")
select(res$values, contains("Model")) %>% colMeans() 
```
Here we promote GBM.   

### Conclusion
I would recommend GBM for now in both regression and classification application.    

This is to be continued with model evaluation.  
### model evaluation  
### model selection / validation. 
## recommendation if necessary  

```{r, echo = FALSE}

```

